{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "tr = pickle.load(open('Dataset/tr.pkl', 'rb'))\n",
    "vl = pickle.load(open('Dataset/vl.pkl', 'rb'))\n",
    "ts = pickle.load(open('Dataset/ts.pkl', 'rb'))\n",
    "\n",
    "print(len(tr), len(vl), len(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "\n",
    "NLP = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_MAX = 404\n",
    "\n",
    "class DS_Imdb(Dataset):\n",
    "    def __init__(self, dat):\n",
    "        self.dat = dat\n",
    "        self.END = NLP('ã€‚')[0].vector\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dat)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sent, lbl  = self.dat[idx]\n",
    "        l = len(sent)\n",
    "        \n",
    "        inp = np.zeros((LEN_MAX, 300))\n",
    "        for i in range(l):\n",
    "            inp[i] = sent[i].vector\n",
    "        for i in range(l, LEN_MAX):\n",
    "            inp[i] = self.END\n",
    "        \n",
    "        return l, inp, lbl\n",
    "\n",
    "ld_tr = DataLoader(DS_Imdb(tr), batch_size=32, shuffle=True)\n",
    "ld_vl = DataLoader(DS_Imdb(vl), batch_size=64)\n",
    "ld_ts = DataLoader(DS_Imdb(ts), batch_size=64)\n",
    "\n",
    "for l, inp, ans in ld_tr:\n",
    "    print(l.shape, inp.shape, ans.shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_LSTM(nn.Module):\n",
    "    def __init__(self, hid_size=256, mode='jump', R=20, K=40, N=5):\n",
    "        super(Model_LSTM, self).__init__()\n",
    "        \n",
    "        self.hid_size = hid_size\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.R = R\n",
    "        self.K = K+1 # includes 0\n",
    "        self.N = N\n",
    "        \n",
    "        self.lstm = nn.LSTM(300, self.hid_size, num_layers=2, \n",
    "                            batch_first=True, dropout=0.2)\n",
    "        self.fc_out = nn.Sequential(*[nn.Linear(self.hid_size, 100), nn.ReLU(), nn.Dropout(0.25), \n",
    "                                      nn.Linear(100, 2)]) \n",
    "        self.fc_jump = nn.Sequential(*[nn.Linear(self.hid_size, self.K)])\n",
    "    \n",
    "    def rollout(self, l, inp, M, policy='sample'):\n",
    "        pbs, outs = [], []\n",
    "        \n",
    "        for t in range(M):\n",
    "            pb = []\n",
    "            pos = 0\n",
    "            \n",
    "            for i in range(self.N):\n",
    "                pos_r = (pos+self.R) if (pos+self.R)<l else l\n",
    "                \n",
    "                if i==0:\n",
    "                    out, (h, c) = self.lstm(inp[:, pos:pos_r])\n",
    "                else:\n",
    "                    out, (h, c) = self.lstm(inp[:, pos:pos_r], (h, c))\n",
    "                out = out[:, -1]\n",
    "            \n",
    "                jp = nn.functional.log_softmax(self.fc_jump(out), dim=1)\n",
    "                if policy=='sample':\n",
    "                    pp = np.random.choice(self.K, p=T.exp(jp.data).cpu().numpy()[0])\n",
    "                else:\n",
    "                    pp = np.argmax(jp.data.cpu().numpy()[0])\n",
    "                \n",
    "                pb.append(jp[:, pp])\n",
    "                if pp==0 or (pos_r+pp-1)>=l:\n",
    "                    break\n",
    "                else:\n",
    "                    pos = (pos_r+pp-1)\n",
    "                \n",
    "            pbs.append(pb)\n",
    "            outs.append(out)\n",
    "        \n",
    "        return pbs, outs\n",
    "    \n",
    "    def forward(self, l, inp, is_pg=True, M=16):\n",
    "        l = l.numpy()\n",
    "        batch_size = inp.shape[0]\n",
    "        \n",
    "        pbs, outs = [], []\n",
    "        for i in range(batch_size):\n",
    "            if is_pg==False:\n",
    "                pb, out = self.rollout(l[i], inp[i:i+1, :l[i]], 1, policy='greedy')\n",
    "            else:\n",
    "                pb, out = self.rollout(l[i], inp[i:i+1, :l[i]], M, policy='sample')\n",
    "            \n",
    "            pbs += pb\n",
    "            outs.append(T.cat(out, dim=0))\n",
    "        \n",
    "        outs = T.cat(outs, dim=0)\n",
    "        outs = self.fc_out(outs)\n",
    "        \n",
    "        return pbs, outs\n",
    "\n",
    "model = Model_LSTM().cuda()\n",
    "\n",
    "pb, out = model(l, inp.float().cuda(), is_pg=False)\n",
    "print(len(pb), out.shape)\n",
    "\n",
    "pb, out = model(l, inp.float().cuda(), is_pg=True)\n",
    "print(len(pb), out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pb[0])\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_PG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Loss_PG, self).__init__()\n",
    "        \n",
    "        self.bl = 0\n",
    "        self.nb = 0\n",
    "    \n",
    "    def forward(self, pb, out, ans):\n",
    "        ans = ans.numpy()\n",
    "        out = np.argmax(out.data.cpu().numpy(), axis=1)\n",
    "        \n",
    "        rwd = [1 if c==True else -1 for c in (out==ans)]\n",
    "        \n",
    "        ls = 0\n",
    "        cnt = 0\n",
    "        for i in range(len(ans)):\n",
    "            for p in pb[i]:\n",
    "                cnt += 1\n",
    "                ls += -p*(rwd[i]-self.bl)\n",
    "        ls /= cnt\n",
    "        \n",
    "        self.bl = (self.bl*self.nb + np.average(rwd))/(self.nb+1)\n",
    "        self.nb += 1\n",
    "        \n",
    "        return ls\n",
    "\n",
    "loss_ce = nn.CrossEntropyLoss().cuda()\n",
    "loss_pg = Loss_PG().cuda()\n",
    "optim = T.optim.Adam(model.parameters(), lr=0.0008)\n",
    "\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStop:\n",
    "    def __init__(self, threshold=10):\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.acc_max = 0\n",
    "        self.cnt = 0\n",
    "\n",
    "    def add(self, acc):\n",
    "        if acc<self.acc_max:\n",
    "            self.cnt += 1\n",
    "        else:\n",
    "            self.cnt = 0\n",
    "            self.acc_max = acc\n",
    "\n",
    "        if self.cnt>=self.threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "ES = EarlyStop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in tqdm(range(EPOCHS)):\n",
    "    ls_ep = 0\n",
    "    \n",
    "    model.train()\n",
    "    with tqdm(ld_tr) as TQ:\n",
    "        for l, inp, ans in TQ:\n",
    "            pb, out = model(l, inp.float().cuda(), is_pg=True)\n",
    "            \n",
    "            ans = ans.numpy()\n",
    "            ans = np.repeat(ans, 16)\n",
    "            ans = T.from_numpy(ans)\n",
    "            \n",
    "            ls_bh = loss_pg(pb, out, ans)+loss_ce(out, ans.cuda())\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            ls_bh.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            ls_bh = ls_bh.cpu().detach().numpy()\n",
    "            TQ.set_postfix(ls_bh='%.3f'%(ls_bh))\n",
    "            ls_ep += ls_bh\n",
    "        \n",
    "        ls_ep /= len(TQ)\n",
    "        print('Ep %d: %.4f' % (e+1, ls_ep))\n",
    "        \n",
    "        T.save(model.state_dict(), 'Model/lstm-jump_%d.pt' % (e+1))\n",
    "        \n",
    "    acc_ep = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with tqdm(ld_vl) as TQ:\n",
    "        for l, inp, ans in TQ:\n",
    "            _, out = model(l, inp.float().cuda(), is_pg=False)\n",
    "            \n",
    "            out = out.cpu().detach().numpy()\n",
    "            out = np.argmax(out, axis=1)\n",
    "            ans = ans.numpy()\n",
    "            \n",
    "            acc_bh = np.average(out==ans)\n",
    "            TQ.set_postfix(acc_bh='%.3f'%(acc_bh))\n",
    "            acc_ep += acc_bh\n",
    "        \n",
    "        acc_ep /= len(TQ)\n",
    "        print('%.4f'%(acc_ep))\n",
    "    \n",
    "    if ES.add(acc_ep)==True:\n",
    "        print('Finish training in ep=%d'%(e+1))\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(T.load('Model/lstm-jump.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ep = 0\n",
    "\n",
    "model.eval()\n",
    "with tqdm(ld_ts) as TQ:\n",
    "    for l, inp, ans in TQ:\n",
    "        _, out = model(l, inp.float().cuda(), is_pg=False)\n",
    "            \n",
    "        out = out.cpu().detach().numpy()\n",
    "        out = np.argmax(out, axis=1)\n",
    "        ans = ans.numpy()\n",
    "            \n",
    "        acc_bh = np.average(out==ans)\n",
    "        TQ.set_postfix(acc_bh='%.3f'%(acc_bh))\n",
    "        acc_ep += acc_bh\n",
    "        \n",
    "    acc_ep /= len(TQ)\n",
    "    print('%.4f'%(acc_ep))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
