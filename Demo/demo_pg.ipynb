{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ds_gen(N=50000, M=12):\n",
    "    dat = []\n",
    "    \n",
    "    for i in range(N):\n",
    "        tmp = []\n",
    "        \n",
    "        for _ in range(M):\n",
    "            tmp.append(np.random.randint(7))\n",
    "        \n",
    "        dat.append([tmp, int(np.average(tmp))])\n",
    "        \n",
    "    return dat\n",
    "\n",
    "tr, vl, ts = ds_gen(), ds_gen(), ds_gen()\n",
    "\n",
    "print(tr[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS_PG(Dataset):\n",
    "    def __init__(self, dat):\n",
    "        self.dat = dat\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dat)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inp = self.dat[idx][0]\n",
    "        lbl = self.dat[idx][1]\n",
    "        \n",
    "        return np.asarray(inp), lbl\n",
    "\n",
    "ld_tr = DataLoader(DS_PG(tr), batch_size=32, shuffle=True)\n",
    "ld_vl = DataLoader(DS_PG(vl), batch_size=64)\n",
    "ld_ts = DataLoader(DS_PG(ts), batch_size=64)\n",
    "\n",
    "for inp, lbl in ld_tr:\n",
    "    print(inp.shape, lbl.shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.mlp = nn.Sequential(*[nn.Linear(12, 32), nn.ReLU(), \n",
    "                                   nn.Linear(32, 8)])\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        out = self.mlp(inp)\n",
    "        \n",
    "        return out\n",
    "\n",
    "model = Model().cuda()\n",
    "\n",
    "out = model(inp.float().cuda())\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_PG(nn.Module):\n",
    "    def __init__(self, M=4):\n",
    "        super(Loss_PG, self).__init__()\n",
    "        \n",
    "        self.M = M\n",
    "        \n",
    "        self.bl = 0\n",
    "        self.nb = 0\n",
    "    \n",
    "    def forward(self, out, lbl):\n",
    "        lbl = lbl.cpu().numpy()\n",
    "        lbl = np.repeat(lbl, self.M)\n",
    "        \n",
    "        out = nn.functional.log_softmax(out, dim=1)\n",
    "        tmp = T.multinomial(T.exp(out), self.M, replacement=True).view((-1, ))\n",
    "        tmp = tmp.data.cpu().numpy()\n",
    "        out = T.cat([out[(i//self.M):(i//self.M)+1, tmp[i]] for i in range(out.shape[0]*self.M)], dim=0)\n",
    "        \n",
    "        rwd = [1 if c==True else -1 for c in (tmp==lbl)]\n",
    "        \n",
    "        ls = 0\n",
    "        for i in range(tmp.shape[0]):\n",
    "            ls += -out[i]*(rwd[i]-self.bl)\n",
    "        ls /= tmp.shape[0]\n",
    "        \n",
    "        self.bl = (self.bl*self.nb + np.average(rwd))/(self.nb+1)\n",
    "        self.nb += 1\n",
    "        \n",
    "        return ls\n",
    "\n",
    "loss_func = Loss_PG(M=2).cuda()\n",
    "optim = T.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "for e in tqdm(range(EPOCHS)):\n",
    "    ls_ep = 0\n",
    "    \n",
    "    model.train()\n",
    "    with tqdm(ld_tr) as TQ:\n",
    "        for inp, lbl in TQ:\n",
    "            out = model(inp.float().cuda())\n",
    "            ls_bh = loss_func(out, lbl.cuda())\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            ls_bh.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            ls_bh = ls_bh.cpu().detach().numpy()\n",
    "            TQ.set_postfix(ls_bh='%.3f'%(ls_bh))\n",
    "            ls_ep += ls_bh\n",
    "        \n",
    "        ls_ep /= len(TQ)\n",
    "        print('Ep %d: %.4f' % (e+1, ls_ep))\n",
    "        \n",
    "    acc_ep = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with tqdm(ld_vl) as TQ:\n",
    "        for inp, lbl in TQ:\n",
    "            out = model(inp.float().cuda())\n",
    "            \n",
    "            out = out.cpu().detach().numpy()\n",
    "            out = np.argmax(out, axis=1)\n",
    "            lbl = lbl.numpy()\n",
    "            \n",
    "            acc_bh = np.average(out==lbl)\n",
    "            TQ.set_postfix(acc_bh='%.3f'%(acc_bh))\n",
    "            acc_ep += acc_bh\n",
    "        \n",
    "        acc_ep /= len(TQ)\n",
    "        print('%.4f'%(acc_ep))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
